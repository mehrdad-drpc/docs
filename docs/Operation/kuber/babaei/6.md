# 06. Install & Update Backup

## Backup Candidates

- Resource Configuration
- ETCD Cluster
- Persist Volumes
  - [Velero] is a Backup and migrate Kubernetes resources and persistent volumes

## Installation, Configuration and Validation

### Objectives

- Design a Kubernetes Cluster
- Choosing Kubernetes Infrastructure
- H.A. Kubernetes Cluster
- Deploy a Kubernetes Cluster
- Cluster E2E Test

### Purpose use of Kubernetes Cluster

#### Education

- MiniKube
- Single-node cluster with Kubeadm/GCP/AWD

#### Development & Testing

- Multi-node cluster with a **Single Master** and **Multiple Workers**
- Setup using kubeadm tool or quick provision on GCP/AWS/AKS

#### Hosting Production Application

- **Hight Available** multi-node cluster with **Multi Master Node**
- Using *kubeadm/KOps* on-perm or GCP/AWS or other supported platform
- Considering for large cluster
  - up to 5k nodes
  - up to 150k PODs in the cluster
  - up to 300k total containers
  - up to 100 pods per node

#### Cloud or On-premise?

- Use Kubeadm/KOps/Kubespray for on-perm
- GKE for GCP
- EKS for AWS
- AKS for Azure

#### Workloads

- How many?
- What kind?
  - Web
  - AI
  - Big Data Analytics

#### Application Resource Requirement

- CPU intensive
- Memory intensive

#### Storage

- High performance - SSD backed storage
- Multiple concurrent connections - Network-based storage
- Persist shared volumes - for shared access across shared PODs

#### Network Traffic

- Continuous Heavy
- Burst

#### Nodes

- Virtual or physical machines

- Master vs worker node
  - Master nodes can host workloads (not recommended)
- Linux x86_64 architecture

#### Large Scale Master Nodes

- Is better to cluster ETCD on different machine

## Choosing Kubernetes Infrastructure

### Turnkey Solution

- Openshift

- Vagrant

### Hosted Solution

- Manged by host
- K8S-as-a-Service
- GKE - Google kubernetes engine
- Amazon elastic container service for kubernetes (EKS)
- Microsoft Azure

## High Availability Kubernetes Cluster

### Tips

> ControlePlane: Master Nodes
> <br>
> DataPlane: Worker Nodes

### API Server

[API-Server] can cluster in `Active Active` mode via a *loadbalancer* (Nginx, Haproxy, ...)

### Control Manager

Control-Manager must be cluster in `Active Passive` mode

### Scheduler

Scheduler must be cluster in `Active Passive` mode

### Cluster Leader Election

A [simple view][simple-view-le] to get better understanding

### ETC

#### [Options for Highly Available Topology][ofhat]

#### [Stacked etcd topology][set]

#### [External etcd topology][eet]

#### Number of ETCD Nodes

| Instances | Fault Tolerance | Usage |
| --- | --- | --- |
| 1 | 0 | no  |
| 2 | 0 | no  |
| 3 | 1 | yes |
| 4 | 1 | no  |
| 5 | 2 | yes |
| 6 | 2 | no  |
| 7 | 3 | yes |

## Deploy Kubernetes Cluster

### Manually

- [Kubernetes the hard way][k8s-thw]
- [Kubeadm][kubeadm-cluster]

#### [Steps of Kubeadm][step-of-kadm]

1. Provision the VMs (master, worker)
2. Select and install CRE (docker, crio, ...)
3. Install kubeadm on all nodes
4. Initialize the cluster (master node)
5. Apply a CLI (calico, flannel, ...) on cluster
6. Join worker nodes to the cluster

### Automation

### Developer Mode Provision

- [vagrant]
- [kind]

## Cluster End2End Test (Validating)

- Full test has around 1K checks - take ages ~ 12h
- Conforming has around 160 checks - enough to be certified 1.5h
  - Network should function for intra-pod communication
  - Service should serve a basic endpoint from pods
  - Service point latency should not very high
  - DNS should provide DNS for services
  - Secret should be consumable in multiple volume in a pod
  - Secret should be consumable from pods in volume in mapping
  - ConfigMap should be consumable from pods in volumes

### Validate Kubernetes Configuration with [Sonobuoy]

```bash
sonobuoy run
sonobuoy status
sonobuoy log
sonobuoy delete
```

## Commands

```bash
# get all resource configuration files from all namespaces
kubectl get --all-namespace -o yaml > all-deploy-services.yaml
```

### Upgrade K8S Cluster

```bash
# upgrade cluster via kubeadm and "-1/+1" strategy

# master nodes
kubeadm upgrade plan
sudo apt install kubeadm=vM.m.p # example v1.29.3
kubeadm upgrade apply vM.m.p
sudo apt install kubectl=vM.m.p # as same as kubeadm version
sudo systemctl restart kubelet
kubectl get nodes

# worker nodes
# execute it from master node
kubectl get nodes
kubectl drain worker-node-01 --ignore-daemonsets
# execute it from worker-node-01
sudo apt install kubeadm=vM.m.p # as same as kubeadm version
sudo apt install kubelet=vM.m.p # as same as kubeadm version
kubeadm upgrade node
sudo systemctl restart kubelet
kubectl uncordon worker-node-01
kubectl get nodes
# do it the same for other nodes

# unhold packages if you would have done before
```

[API-Server]: /docs/site/assets/images/S06-k8s-cluster-api_server.png
[simple-view-le]: ../../../assets/kuber/babaei/S06-k8s-cluster-leader-election.png

[Velero]: https://velero.io/
[ofhat]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/
[set]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology
[eet]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology
[k8s-thw]: https://github.com/kelseyhightower/kubernetes-the-hard-way
[kubeadm-cluster]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
[step-of-kadm]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#instructions
[vagrant]: https://developer.hashicorp.com/vagrant
[kind]: https://kind.sigs.k8s.io/
[Sonobuoy]: https://sonobuoy.io/
